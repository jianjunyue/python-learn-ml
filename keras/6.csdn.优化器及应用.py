# SGD:批量随机梯度下降
# Agagrad：主要优势在于不需要人为的调节学习率，它会自动调节。它的缺点：随着迭代次数的增加，学习率也会越来越小，最终趋向于0，模型训练结束；不适用模型需要次数比较多的情况
# Adadelta：Agagrad的改进，Adadelta不需要适用学习率，可以得到一个很好的效果
# RMSprop：Agagrad的改进，RMSprop不会出现学习率越来越低的问题，而且也能自己调节学习率，可以得到一个比较好的效果
# Adam：一种常用优化器，Adam会存储之前衰减的平方梯度，同时它也会保存之前的衰减的梯度，经过一些处理之后再用来更新权重W
Adamax
Nadam
TFOpimizer